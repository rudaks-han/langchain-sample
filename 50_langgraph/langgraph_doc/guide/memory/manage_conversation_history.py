from dotenv import load_dotenv
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState, StateGraph, START, END
from langgraph.prebuilt import ToolNode

load_dotenv()

memory = MemorySaver()


@tool
def search(query: str):
    """Call to surf the web."""
    return "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."


tools = [search]
tool_node = ToolNode(tools)
model = ChatOpenAI(model_name="gpt-4o-mini")
bound_model = model.bind_tools(tools)


def should_continue(state: MessagesState):
    """Return the next node to execute."""
    last_message = state["messages"][-1]
    # ë§Œì¼ í•¨ìˆ˜ í˜¸ì¶œì´ ì—†ë‹¤ë©´, ëë‚¸ë‹¤.
    if not last_message.tool_calls:
        return END
    # ë§Œì¼ í•¨ìˆ˜ í˜¸ì¶œì´ ìˆë‹¤ë©´, ê³„ì†í•œë‹¤.
    return "action"


# ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.
def call_model(state: MessagesState):
    response = bound_model.invoke(state["messages"])
    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤. ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤.
    return {"messages": response}


workflow = StateGraph(MessagesState)

# ë‘ ê°œì˜ ë…¸ë“œê°€ ì„œë¡œë¥¼ ìˆœí™˜í•˜ë„ë¡ ì •ì˜í•œë‹¤.
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)

# ì§„ì…ì ì„ 'agent'ë¡œ ì„¤ì •í•œë‹¤.
# ì´ê²ƒì€ ì´ ë…¸ë“œê°€ ì²˜ìŒìœ¼ë¡œ í˜¸ì¶œë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
workflow.add_edge(START, "agent")

# ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•œë‹¤.
workflow.add_conditional_edges(
    # ìš°ì„ , ì‹œì‘ ë…¸ë“œë¥¼ ì •ì˜í•œë‹¤. 'agent'ë¥¼ ì‚¬ìš©í•œë‹¤.
    # ì´ê²ƒì€ 'agent' ë…¸ë“œê°€ í˜¸ì¶œëœ í›„ì— í˜¸ì¶œë˜ëŠ” ì—£ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.
    "agent",
    # ë‹¤ìŒìœ¼ë¡œ, ë‹¤ìŒì— í˜¸ì¶œë  ë…¸ë“œë¥¼ ê²°ì •í•  í•¨ìˆ˜ë¥¼ ì „ë‹¬í•œë‹¤.
    should_continue,
    # ë‹¤ìŒìœ¼ë¡œ, ê²½ë¡œ ë§µì„ ì „ë‹¬í•œë‹¤. ì´ ì—£ì§€ê°€ ê°ˆ ìˆ˜ ìˆëŠ” ëª¨ë“  ë…¸ë“œë“¤ì´ë‹¤.
    ["action", END],
)

# toolsì—ì„œ agentë¡œì˜ ì¼ë°˜ ì—£ì§€ë¥¼ ì¶”ê°€í•œë‹¤.
# ì´ê²ƒì€ toolsê°€ í˜¸ì¶œëœ í›„ì— agent ë…¸ë“œê°€ í˜¸ì¶œëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
workflow.add_edge("action", "agent")

# ë§ˆì§€ë§‰ìœ¼ë¡œ, ì»´íŒŒì¼í•œë‹¤!
# ì´ê²ƒì€ LangChain Runnableë¡œ ì»´íŒŒì¼ëœë‹¤.
# ì´ê²ƒì€ ë‹¤ë¥¸ Runnableì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
app = workflow.compile(checkpointer=memory)

from langchain_core.messages import HumanMessage

config = {"configurable": {"thread_id": "2"}}
input_message = HumanMessage(content="ì•ˆë…•! ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()


input_message = HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­ì•¼?")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

messages = app.get_state(config).values["messages"]
print(messages)

from langchain_core.messages import RemoveMessage

app.update_state(config, {"messages": RemoveMessage(id=messages[0].id)})

messages = app.get_state(config).values["messages"]
print(messages)
