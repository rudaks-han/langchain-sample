from typing import Literal

from dotenv import load_dotenv
from langchain_core.messages import RemoveMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState, StateGraph, START, END
from langgraph.prebuilt import ToolNode

load_dotenv()

memory = MemorySaver()


@tool
def search(query: str):
    """Call to surf the web."""
    return "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."


tools = [search]
tool_node = ToolNode(tools)
model = ChatOpenAI(model_name="gpt-4o-mini")
bound_model = model.bind_tools(tools)


def delete_messages(state):
    messages = state["messages"]
    if len(messages) > 3:
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:-3]]}


# ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.
def call_model(state: MessagesState):
    response = bound_model.invoke(state["messages"])
    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤. ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤.
    return {"messages": response}


# ë°”ë¡œ ëë‚´ëŠ” ëŒ€ì‹ ì— delete_messagesë¥¼ í˜¸ì¶œí•˜ëŠ” ë¡œì§ì„ ìˆ˜ì •í•  í•„ìš”ê°€ ìˆë‹¤.
def should_continue(state: MessagesState) -> Literal["action", "delete_messages"]:
    """Return the next node to execute."""
    last_message = state["messages"][-1]
    # ë§Œì¼ í•¨ìˆ˜ í˜¸ì¶œì´ ì—†ë‹¤ë©´, ëë‚¸ë‹¤.
    if not last_message.tool_calls:
        return "delete_messages"
    # ë§Œì¼ í•¨ìˆ˜ í˜¸ì¶œì´ ìˆë‹¤ë©´, ê³„ì†í•œë‹¤.
    return "action"


workflow = StateGraph(MessagesState)
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)

# ì´ê²ƒì´ ìš°ë¦¬ê°€ ì •ì˜í•˜ëŠ” ìƒˆë¡œìš´ ë…¸ë“œì´ë‹¤.
workflow.add_node(delete_messages)


workflow.add_edge(START, "agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
)
workflow.add_edge("action", "agent")

# ì¶”ê°€í•˜ê³  ìˆëŠ” ìƒˆë¡œìš´ ì—£ì§€ì´ë‹¤. ë©”ì‹œì§€ë¥¼ ì‚­ì œí•œ í›„ì— ëë‚¸ë‹¤.
workflow.add_edge("delete_messages", END)
app = workflow.compile(checkpointer=memory)

from langchain_core.messages import HumanMessage

config = {"configurable": {"thread_id": "3"}}
input_message = HumanMessage(content="ì•ˆë…•! ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    print([(message.type, message.content) for message in event["messages"]])


input_message = HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­ì•¼?")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    print([(message.type, message.content) for message in event["messages"]])

messages = app.get_state(config).values["messages"]
print(messages)
